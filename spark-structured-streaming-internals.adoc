== Internals of Streaming Datasets

NOTE: The page is to keep notes about how to guide readers through the codebase and may disappear if merged with the other pages or become an intro page.

. <<DataStreamReader, DataStreamReader and Streaming Data Source>>
. <<data-source-resolution, Streaming Dataset, Data Source Resolution and Logical Query Plan>>

=== [[DataStreamReader]] DataStreamReader and Streaming Data Source

It all starts with `SparkSession.readStream` method which lets you define a <<spark-sql-streaming-Source.adoc#, streaming source>> in a *stream processing graph* (a _dataflow graph_).

[source, scala]
----
import org.apache.spark.sql.SparkSession
assert(spark.isInstanceOf[SparkSession])

val reader = spark.readStream

import org.apache.spark.sql.streaming.DataStreamReader
assert(reader.isInstanceOf[DataStreamReader])
----

`SparkSession.readStream` method creates a <<spark-sql-streaming-DataStreamReader.adoc#, DataStreamReader>>.

The fluent API of `DataStreamReader` allows you to describe the input data source (e.g. <<spark-sql-streaming-DataStreamReader.adoc#format, DataStreamReader.format>> and <<spark-sql-streaming-DataStreamReader.adoc#options, DataStreamReader.options>>) using method chaining (with the goal of making the readability of the source code close to that of ordinary written prose, essentially creating a domain-specific language within the interface. See https://en.wikipedia.org/wiki/Fluent_interface[Fluent interface] article in Wikipedia).

[source, scala]
----
reader
  .format("csv")
  .option("delimiter", "|")
----

There are a couple of built-in data source formats. Their names are the names of the corresponding `DataStreamReader` methods and so act like shortcuts of `DataStreamReader.format` (where you have to specify the format by name), i.e. <<spark-sql-streaming-DataStreamReader.adoc#csv, csv>>, <<spark-sql-streaming-DataStreamReader.adoc#json, json>>, <<spark-sql-streaming-DataStreamReader.adoc#orc, orc>>, <<spark-sql-streaming-DataStreamReader.adoc#parquet, parquet>> and <<spark-sql-streaming-DataStreamReader.adoc#text, text>>, followed by <<spark-sql-streaming-DataStreamReader.adoc#load, DataStreamReader.load>>.

You may also want to use <<spark-sql-streaming-DataStreamReader.adoc#schema, DataStreamReader.schema>> method to specify the schema of the streaming data source.

[source, scala]
----
reader.schema("a INT, b STRING")
----

In the end, you use <<spark-sql-streaming-DataStreamReader.adoc#load, DataStreamReader.load>> method that simply creates a streaming Dataset (the good ol' Dataset that you may have already used in Spark SQL).

[source, scala]
----
val input = reader.format("csv").load("data/streaming")

import org.apache.spark.sql.DataFrame
assert(input.isInstanceOf[DataFrame])
----

The Dataset has the `isStreaming` property enabled that is basically the only way you could distinguish streaming Datasets from regular, batch Datasets.

[source, scala]
----
assert(input.isStreaming)
----

In other words, Spark Structured Streaming is designed to extend the features of Spark SQL and let your structured queries be streaming queries.

=== [[data-source-resolution]] Streaming Dataset, Data Source Resolution and Logical Query Plan

Being curious about the internals of streaming Datasets is where you start...seeing numbers not humans (sorry, couldn't resist drawing the comparison between https://en.wikipedia.org/wiki/The_Matrix[Matrix the movie] and the internals of Spark Structured Streaming).

Whenever you create a Dataset (be it batch in Spark SQL or streaming in Spark Structured Streaming) is when you create a *logical query plan* using the *high-level Dataset DSL*.

A logical query plan is made up of logical operators.

Spark Structured Streaming gives you two logical operators to represent streaming sources, i.e. <<spark-sql-streaming-StreamingRelationV2.adoc#, StreamingRelationV2>> and <<spark-sql-streaming-StreamingRelation.adoc#, StreamingRelation>>.

When <<spark-sql-streaming-DataStreamReader.adoc#load, DataStreamReader.load>> method is executed, `load` first looks up the requested data source (that you specified using <<spark-sql-streaming-DataStreamReader.adoc#format, DataStreamReader.format>>) and creates an instance of it (_instantiation_). That'd be *data source resolution* step (that I described in...FIXME).

`DataStreamReader.load` is where you can find the intersection of the former <<spark-sql-streaming-micro-batch-processing.adoc#, Micro-Batch Stream Processing>> V1 API with the new <<spark-sql-streaming-continuous-stream-processing.adoc#, Continuous Stream Processing>> V2 API.

For <<spark-sql-streaming-MicroBatchReadSupport.adoc#, MicroBatchReadSupport>> or <<spark-sql-streaming-ContinuousReadSupport.adoc#, ContinuousReadSupport>> data sources, `DataStreamReader.load` creates a logical query plan with a <<spark-sql-streaming-StreamingRelationV2.adoc#, StreamingRelationV2>> leaf logical operator. That is the new *V2 code path*.

.StreamingRelationV2 Logical Operator for Data Source V2
[source, scala]
----
// rate data source is V2
val rates = spark.readStream.format("rate").load
val plan = rates.queryExecution.logical
scala> println(plan.numberedTreeString)
00 StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2ed03b1a, rate, [timestamp#12, value#13L]
----

For all other types of streaming data sources, `DataStreamReader.load` creates a logical query plan with a <<spark-sql-streaming-StreamingRelation.adoc#, StreamingRelation>> leaf logical operator. That is the former *V1 code path*.

.StreamingRelation Logical Operator for Data Source V1
[source, scala]
----
// text data source is V1
val texts = spark.readStream.format("text").load("data/streaming")
val plan = texts.queryExecution.logical
scala> println(plan.numberedTreeString)
00 StreamingRelation DataSource(org.apache.spark.sql.SparkSession@35edd886,text,List(),None,List(),None,Map(path -> data/streaming),None), FileSource[data/streaming], [value#18]
----
