== Internals of Streaming Datasets

NOTE: The page is to keep notes about how to guide readers through the codebase and may disappear if merged with the other pages or become an intro page.

. <<DataStreamReader, DataStreamReader and Streaming Data Source>>
. <<data-source-resolution, Streaming Dataset, Data Source Resolution and Logical Query Plan>>
. <<dataset, Dataset API -- High-Level DSL to Build Logical Query Plan>>

=== [[DataStreamReader]] DataStreamReader and Streaming Data Source

It all starts with `SparkSession.readStream` method which lets you define a <<spark-sql-streaming-Source.adoc#, streaming source>> in a *stream processing graph* (a _dataflow graph_).

[source, scala]
----
import org.apache.spark.sql.SparkSession
assert(spark.isInstanceOf[SparkSession])

val reader = spark.readStream

import org.apache.spark.sql.streaming.DataStreamReader
assert(reader.isInstanceOf[DataStreamReader])
----

`SparkSession.readStream` method creates a <<spark-sql-streaming-DataStreamReader.adoc#, DataStreamReader>>.

The fluent API of `DataStreamReader` allows you to describe the input data source (e.g. <<spark-sql-streaming-DataStreamReader.adoc#format, DataStreamReader.format>> and <<spark-sql-streaming-DataStreamReader.adoc#options, DataStreamReader.options>>) using method chaining (with the goal of making the readability of the source code close to that of ordinary written prose, essentially creating a domain-specific language within the interface. See https://en.wikipedia.org/wiki/Fluent_interface[Fluent interface] article in Wikipedia).

[source, scala]
----
reader
  .format("csv")
  .option("delimiter", "|")
----

There are a couple of built-in data source formats. Their names are the names of the corresponding `DataStreamReader` methods and so act like shortcuts of `DataStreamReader.format` (where you have to specify the format by name), i.e. <<spark-sql-streaming-DataStreamReader.adoc#csv, csv>>, <<spark-sql-streaming-DataStreamReader.adoc#json, json>>, <<spark-sql-streaming-DataStreamReader.adoc#orc, orc>>, <<spark-sql-streaming-DataStreamReader.adoc#parquet, parquet>> and <<spark-sql-streaming-DataStreamReader.adoc#text, text>>, followed by <<spark-sql-streaming-DataStreamReader.adoc#load, DataStreamReader.load>>.

You may also want to use <<spark-sql-streaming-DataStreamReader.adoc#schema, DataStreamReader.schema>> method to specify the schema of the streaming data source.

[source, scala]
----
reader.schema("a INT, b STRING")
----

In the end, you use <<spark-sql-streaming-DataStreamReader.adoc#load, DataStreamReader.load>> method that simply creates a streaming Dataset (the good ol' Dataset that you may have already used in Spark SQL).

[source, scala]
----
val input = reader.format("csv").load("data/streaming")

import org.apache.spark.sql.DataFrame
assert(input.isInstanceOf[DataFrame])
----

The Dataset has the `isStreaming` property enabled that is basically the only way you could distinguish streaming Datasets from regular, batch Datasets.

[source, scala]
----
assert(input.isStreaming)
----

In other words, Spark Structured Streaming is designed to extend the features of Spark SQL and let your structured queries be streaming queries.

=== [[data-source-resolution]] Streaming Dataset, Data Source Resolution and Logical Query Plan

Being curious about the internals of streaming Datasets is where you start...seeing numbers not humans (sorry, couldn't resist drawing the comparison between https://en.wikipedia.org/wiki/The_Matrix[Matrix the movie] and the internals of Spark Structured Streaming).

Whenever you create a Dataset (be it batch in Spark SQL or streaming in Spark Structured Streaming) is when you create a *logical query plan* using the *high-level Dataset DSL*.

A logical query plan is made up of logical operators.

Spark Structured Streaming gives you two logical operators to represent streaming sources, i.e. <<spark-sql-streaming-StreamingRelationV2.adoc#, StreamingRelationV2>> and <<spark-sql-streaming-StreamingRelation.adoc#, StreamingRelation>>.

When <<spark-sql-streaming-DataStreamReader.adoc#load, DataStreamReader.load>> method is executed, `load` first looks up the requested data source (that you specified using <<spark-sql-streaming-DataStreamReader.adoc#format, DataStreamReader.format>>) and creates an instance of it (_instantiation_). That'd be *data source resolution* step (that I described in...FIXME).

`DataStreamReader.load` is where you can find the intersection of the former <<spark-sql-streaming-micro-batch-processing.adoc#, Micro-Batch Stream Processing>> V1 API with the new <<spark-sql-streaming-continuous-stream-processing.adoc#, Continuous Stream Processing>> V2 API.

For <<spark-sql-streaming-MicroBatchReadSupport.adoc#, MicroBatchReadSupport>> or <<spark-sql-streaming-ContinuousReadSupport.adoc#, ContinuousReadSupport>> data sources, `DataStreamReader.load` creates a logical query plan with a <<spark-sql-streaming-StreamingRelationV2.adoc#, StreamingRelationV2>> leaf logical operator. That is the new *V2 code path*.

.StreamingRelationV2 Logical Operator for Data Source V2
[source, scala]
----
// rate data source is V2
val rates = spark.readStream.format("rate").load
val plan = rates.queryExecution.logical
scala> println(plan.numberedTreeString)
00 StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2ed03b1a, rate, [timestamp#12, value#13L]
----

For all other types of streaming data sources, `DataStreamReader.load` creates a logical query plan with a <<spark-sql-streaming-StreamingRelation.adoc#, StreamingRelation>> leaf logical operator. That is the former *V1 code path*.

.StreamingRelation Logical Operator for Data Source V1
[source, scala]
----
// text data source is V1
val texts = spark.readStream.format("text").load("data/streaming")
val plan = texts.queryExecution.logical
scala> println(plan.numberedTreeString)
00 StreamingRelation DataSource(org.apache.spark.sql.SparkSession@35edd886,text,List(),None,List(),None,Map(path -> data/streaming),None), FileSource[data/streaming], [value#18]
----

=== [[dataset]] Dataset API -- High-Level DSL to Build Logical Query Plan

With the streaming Dataset created, you can now use all the methods of `Dataset` API, including but not limited to the following operators:

* <<spark-sql-streaming-Dataset-operators.adoc#dropDuplicates, Dataset.dropDuplicates>> for streaming deduplication

* <<spark-sql-streaming-Dataset-operators.adoc#groupBy, Dataset.groupBy>> and <<spark-sql-streaming-Dataset-operators.adoc#groupByKey, Dataset.groupByKey>> for streaming aggregation

* <<spark-sql-streaming-Dataset-operators.adoc#withWatermark, Dataset.withWatermark>> for event time watermark

Please note that a streaming Dataset is a regular Dataset (_with some streaming-related limitations_).

[source, scala]
----
val rates = spark
  .readStream
  .format("rate")
  .load
import java.sql.Timestamp
val countByTime = rates
  .as[(Timestamp, Long)]
  .groupByKey(_._1)
  .count

import org.apache.spark.sql.Dataset
assert(countByTime.isInstanceOf[Dataset[_]]))
----

The point is to understand that the Dataset API is a domain-specific language (DSL) to build a more sophisticated stream processing graph that you could also build using the low-level logical operators directly.

Use <<spark-sql-streaming-Dataset-operators.adoc#explain, Dataset.explain>> to learn the underlying logical and physical query plans.

[source, scala]
----
assert(countByTime.isStreaming)

scala> countByTime.explain(extended = true)
== Parsed Logical Plan ==
Aggregate [value#39], [value#39, count(1) AS count(1)#43L]
+- AppendColumns <function1>, class scala.Tuple2, [StructField(_1,TimestampType,true), StructField(_2,LongType,false)], newInstance(class scala.Tuple2), [staticinvoke(class org.apache.spark.sql.catalyst.util.DateTimeUtils$, TimestampType, fromJavaTimestamp, input[0, java.sql.Timestamp, true], true, false) AS value#39]
   +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@60eed3d7, rate, [timestamp#20, value#21L]

== Analyzed Logical Plan ==
value: timestamp, count(1): bigint
Aggregate [value#39], [value#39, count(1) AS count(1)#43L]
+- AppendColumns <function1>, class scala.Tuple2, [StructField(_1,TimestampType,true), StructField(_2,LongType,false)], newInstance(class scala.Tuple2), [staticinvoke(class org.apache.spark.sql.catalyst.util.DateTimeUtils$, TimestampType, fromJavaTimestamp, input[0, java.sql.Timestamp, true], true, false) AS value#39]
   +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@60eed3d7, rate, [timestamp#20, value#21L]

== Optimized Logical Plan ==
Aggregate [value#39], [value#39, count(1) AS count(1)#43L]
+- Project [value#39]
   +- AppendColumns <function1>, class scala.Tuple2, [StructField(_1,TimestampType,true), StructField(_2,LongType,false)], newInstance(class scala.Tuple2), [staticinvoke(class org.apache.spark.sql.catalyst.util.DateTimeUtils$, TimestampType, fromJavaTimestamp, input[0, java.sql.Timestamp, true], true, false) AS value#39]
      +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@60eed3d7, rate, [timestamp#20, value#21L]

== Physical Plan ==
*(4) HashAggregate(keys=[value#39], functions=[count(1)], output=[value#39, count(1)#43L])
+- StateStoreSave [value#39], state info [ checkpoint = <unknown>, runId = 29feaa73-8482-4584-804b-442a57b07ce4, opId = 0, ver = 0, numPartitions = 200], Append, 0, 2
   +- *(3) HashAggregate(keys=[value#39], functions=[merge_count(1)], output=[value#39, count#48L])
      +- StateStoreRestore [value#39], state info [ checkpoint = <unknown>, runId = 29feaa73-8482-4584-804b-442a57b07ce4, opId = 0, ver = 0, numPartitions = 200], 2
         +- *(2) HashAggregate(keys=[value#39], functions=[merge_count(1)], output=[value#39, count#48L])
            +- Exchange hashpartitioning(value#39, 200)
               +- *(1) HashAggregate(keys=[value#39], functions=[partial_count(1)], output=[value#39, count#48L])
                  +- *(1) Project [value#39]
                     +- AppendColumns <function1>, newInstance(class scala.Tuple2), [staticinvoke(class org.apache.spark.sql.catalyst.util.DateTimeUtils$, TimestampType, fromJavaTimestamp, input[0, java.sql.Timestamp, true], true, false) AS value#39]
                        +- StreamingRelation rate, [timestamp#20, value#21L]
----

Or go pro and talk to `QueryExecution` directly.

[source, scala]
----
val plan = countByTime.queryExecution.logical
scala> println(plan.numberedTreeString)
00 Aggregate [value#39], [value#39, count(1) AS count(1)#43L]
01 +- AppendColumns <function1>, class scala.Tuple2, [StructField(_1,TimestampType,true), StructField(_2,LongType,false)], newInstance(class scala.Tuple2), [staticinvoke(class org.apache.spark.sql.catalyst.util.DateTimeUtils$, TimestampType, fromJavaTimestamp, input[0, java.sql.Timestamp, true], true, false) AS value#39]
02    +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@60eed3d7, rate, [timestamp#20, value#21L]
----
