== [[StateStore]] StateStore Contract -- Kay-Value Store for State Management

`StateStore` is the <<contract, contract>> of a <<version, versioned>> and possibly fault-tolerant <<implementations, key-value stores>> for state management (e.g. of <<spark-sql-streaming-Dataset-operators.adoc#, streaming aggregations>>).

`StateStore` describes a key-value store that lives on every executor (across the nodes in a Spark cluster) for persistent keyed aggregates.

`StateStore` is identified with the <<id, aggregating operator id and the partition id>> (among other properties for state store identification).

TIP: Read the motivation and design in https://docs.google.com/document/d/1-ncawFx8JS5Zyfq1HAEGBx56RDet9wfVp_hDM8ZL254/edit[State Store for Streaming Aggregations].

[[contract]]
.StateStore Contract
[cols="1m,2",options="header",width="100%"]
|===
| Method
| Description

| abort
a| [[abort]]

[source, scala]
----
abort(): Unit
----

Aborts the state changes

Used when:

* `StateStoreOps` implicit class is requested to <<spark-sql-streaming-StateStoreOps.adoc#mapPartitionsWithStateStore, mapPartitionsWithStateStore>> (when the state store has not been <<hasCommitted, committed>> for a task that finishes, possibly with an error)

* `StateStoreHandler` is requested to `abortIfNeeded` (when the state store has not been <<hasCommitted, committed>> for a task that finishes, possibly with an error)

| commit
a| [[commit]]

[source, scala]
----
commit(): Long
----

Commits state changes

Used when:

* <<spark-sql-streaming-FlatMapGroupsWithStateExec.adoc#doExecute, FlatMapGroupsWithStateExec>>, <<spark-sql-streaming-StreamingDeduplicateExec.adoc#doExecute, StreamingDeduplicateExec>> and <<spark-sql-streaming-StreamingGlobalLimitExec.adoc#doExecute, StreamingGlobalLimitExec>> physical operators are executed (right after all rows in a partition have been processed successfully)

* `StreamingAggregationStateManagerBaseImpl` is requested to <<spark-sql-streaming-StreamingAggregationStateManagerBaseImpl.adoc#commit, commit (changes to) a state store>> (exclusively when <<spark-sql-streaming-StateStoreSaveExec.adoc#, StateStoreSaveExec>> physical operator is executed)

* `StateStoreHandler` is requested to <<spark-sql-streaming-StateStoreHandler.adoc#commit, commit changes to a state store>>

| get
a| [[get]]

[source, scala]
----
get(key: UnsafeRow): UnsafeRow
----

| getRange
a| [[getRange]]

[source, scala]
----
getRange(start: Option[UnsafeRow], end: Option[UnsafeRow]): Iterator[UnsafeRowPair]
----

| hasCommitted
a| [[hasCommitted]]

[source, scala]
----
hasCommitted: Boolean
----

Used when:

* `StateStoreOps` implicit class is requested to <<spark-sql-streaming-StateStoreOps.adoc#mapPartitionsWithStateStore, mapPartitionsWithStateStore>> (when a task finishes, possibly with an error, to check whether to <<abort, abort>> state updates)

* `StateStoreHandler` is requested to `abortIfNeeded` (when a task finishes, possibly with an error, to check whether to <<abort, abort>> state updates)

| id
a| [[id]]

[source, scala]
----
id: StateStoreId
----

The ID of the state store (for logging purposes only)

Used when:

* `HDFSBackedStateStore` state store is requested for the string identifier (`toString`)

* `StateStoreHandler` is requested to `abortIfNeeded` and `getStateStore`

| iterator
a| [[iterator]]

[source, scala]
----
iterator(): Iterator[UnsafeRowPair]
----

| metrics
a| [[metrics]]

[source, scala]
----
metrics: StateStoreMetrics
----

<<spark-sql-streaming-StateStoreMetrics.adoc#, StateStoreMetrics>>

Used when:

* `StateStoreWriter` stateful physical operator is requested to <<spark-sql-streaming-StateStoreWriter.adoc#setStoreMetrics, setStoreMetrics>>

* `StateStoreHandler` is requested to `commit` and for the `metrics`

| put
a| [[put]]

[source, scala]
----
put(key: UnsafeRow, value: UnsafeRow): Unit
----

Stores a given value for a given non-null key

Used when:

* <<spark-sql-streaming-StreamingDeduplicateExec.adoc#, StreamingDeduplicateExec>> and <<spark-sql-streaming-StreamingGlobalLimitExec.adoc#, StreamingGlobalLimitExec>> physical operators are executed

* `StateManagerImplBase` is requested to `putState`

* <<spark-sql-streaming-StreamingAggregationStateManagerImplV1.adoc#put, StreamingAggregationStateManagerImplV1>> and <<spark-sql-streaming-StreamingAggregationStateManagerImplV2.adoc#put, StreamingAggregationStateManagerImplV2>> are requested to store a row in a state store

* `KeyToNumValuesStore` and `KeyWithIndexToValueStore` are requested to `put`

| remove
a| [[remove]]

[source, scala]
----
remove(key: UnsafeRow): Unit
----

Removes a given key from the state store

Used when:

* Physical operators with `WatermarkSupport` are requested to <<spark-sql-streaming-WatermarkSupport.adoc#removeKeysOlderThanWatermark, removeKeysOlderThanWatermark>>

* `StateManagerImplBase` is requested to `removeState`

* `StreamingAggregationStateManagerBaseImpl` is requested to <<spark-sql-streaming-StreamingAggregationStateManagerBaseImpl.adoc#remove, remove a key from a state store>>

* `KeyToNumValuesStore` is requested to `remove` a key

* `KeyWithIndexToValueStore` is requested to `remove` and `removeAllValues`

| version
a| [[version]]

[source, scala]
----
version: Long
----

Used when:

* `HDFSBackedStateStore` state store is requested for <<spark-sql-streaming-HDFSBackedStateStore.adoc#newVersion, newVersion>>

* `MemoryStateStore` state store is requested to `commit`
|===

[[implementations]]
.StateStores
[cols="1,2",options="header",width="100%"]
|===
| StateStore
| Description

| <<spark-sql-streaming-HDFSBackedStateStore.adoc#, HDFSBackedStateStore>>
| [[HDFSBackedStateStore]] `StateStore` that uses a HDFS-compatible file system for state persistence

| <<spark-sql-streaming-MemoryStateStore.adoc#, MemoryStateStore>>
| [[MemoryStateStore]]
|===

[[internal-registries]]
.StateStore's Internal Registries and Counters
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[loadedProviders]] `loadedProviders`
| Registry of link:spark-sql-streaming-StateStoreProvider.adoc[StateStoreProviders] per `StateStoreProviderId`

Used in...FIXME

| [[_coordRef]] `_coordRef`
| link:spark-sql-streaming-StateStoreCoordinatorRef.adoc[StateStoreCoordinatorRef] (a `RpcEndpointRef` to link:spark-sql-streaming-StateStoreCoordinator.adoc[StateStoreCoordinator]).

Used in...FIXME
|===

NOTE: `StateStore` was introduced in https://github.com/apache/spark/commit/8c826880f5eaa3221c4e9e7d3fece54e821a0b98[[SPARK-13809\][SQL\] State store for streaming aggregations].

=== [[coordinatorRef]] Creating StateStoreCoordinatorRef (for Executors) -- `coordinatorRef` Internal Method

CAUTION: FIXME

=== [[unload]] Removing StateStoreProvider From Provider Registry -- `unload` Internal Method

CAUTION: FIXME

=== [[verifyIfStoreInstanceActive]] `verifyIfStoreInstanceActive` Internal Method

CAUTION: FIXME

=== [[reportActiveStoreInstance]] Announcing New StateStoreProvider -- `reportActiveStoreInstance` Internal Method

[source, scala]
----
reportActiveStoreInstance(storeProviderId: StateStoreProviderId): Unit
----

`reportActiveStoreInstance` takes the current host and executorId (from `BlockManager`) and requests `StateStoreCoordinatorRef` to link:spark-sql-streaming-StateStoreCoordinatorRef.adoc#reportActiveInstance[reportActiveInstance].

NOTE: `reportActiveStoreInstance` uses `SparkEnv` to access the current `BlockManager`.

You should see the following INFO message in the logs:

```
Reported that the loaded instance [storeProviderId] is active
```

NOTE: `reportActiveStoreInstance` is used exclusively when `StateStore` is requested to <<get, find the StateStore by StateStoreProviderId>>.

=== [[numKeys]] `numKeys` Method

CAUTION: FIXME

=== [[get]] Finding StateStore by StateStoreProviderId -- `get` Method

[source, scala]
----
get(
  storeProviderId: StateStoreProviderId,
  keySchema: StructType,
  valueSchema: StructType,
  indexOrdinal: Option[Int],
  version: Long,
  storeConf: StateStoreConf,
  hadoopConf: Configuration): StateStore
----

`get` finds `StateStore` for `StateStoreProviderId`.

Internally, `get` looks up the `StateStoreProvider` (for `storeProviderId`) in <<loadedProviders, loadedProviders>> registry. If unavailable, `get` link:spark-sql-streaming-StateStoreProvider.adoc#createAndInit[creates and initializes one].

`get` will also <<startMaintenanceIfNeeded, start the periodic maintenance task>> (unless already started) and <<reportActiveStoreInstance, announce the new StateStoreProvider>>.

In the end, `get` link:spark-sql-streaming-StateStoreProvider.adoc#getStore[gets] the `StateStore` (for the `version`).

NOTE: `get` is used exclusively when `StateStoreRDD` link:spark-sql-streaming-StateStoreRDD.adoc#compute[is computed].

=== [[startMaintenanceIfNeeded]] Starting Periodic Maintenance Task (Unless Already Started) -- `startMaintenanceIfNeeded` Internal Method

[source, scala]
----
startMaintenanceIfNeeded(): Unit
----

`startMaintenanceIfNeeded` schedules <<MaintenanceTask, MaintenanceTask>> to start after and every link:spark-sql-streaming-properties.adoc#spark.sql.streaming.stateStore.maintenanceInterval[spark.sql.streaming.stateStore.maintenanceInterval] (defaults to `60s`).

NOTE: `startMaintenanceIfNeeded` does nothing when the maintenance task has already been started and is still running.

NOTE: `startMaintenanceIfNeeded` is used exclusively when `StateStore` is requested to <<get, find the StateStore by StateStoreProviderId>>.

=== [[MaintenanceTask]] `MaintenanceTask` Daemon Thread

`MaintenanceTask` is a daemon thread that <<doMaintenance, triggers maintenance work of every registered StateStoreProvider>>.

When an error occurs, `MaintenanceTask` clears <<loadedProviders, loadedProviders>> registry.

`MaintenanceTask` is scheduled on *state-store-maintenance-task* thread pool.

NOTE: Use link:spark-sql-streaming-properties.adoc#spark.sql.streaming.stateStore.maintenanceInterval[spark.sql.streaming.stateStore.maintenanceInterval] Spark property (default: `60s`) to control the initial delay and how often the thread should be executed.

=== [[doMaintenance]] Triggering Maintenance of Registered StateStoreProviders -- `doMaintenance` Internal Method

[source, scala]
----
doMaintenance(): Unit
----

Internally, `doMaintenance` prints the following DEBUG message to the logs:

```
DEBUG Doing maintenance
```

`doMaintenance` then requests every link:spark-sql-streaming-StateStoreProvider.adoc[StateStoreProvider] (registered in <<loadedProviders, loadedProviders>>) to link:spark-sql-streaming-StateStoreProvider.adoc#doMaintenance[do its own internal maintenance] (only when a `StateStoreProvider` <<verifyIfStoreInstanceActive, is still active>>).

When a `StateStoreProvider` is <<verifyIfStoreInstanceActive, inactive>>, `doMaintenance` <<unload, removes it from the provider registry>> and prints the following INFO message to the logs:

```
INFO Unloaded [provider]
```

NOTE: `doMaintenance` is used exclusively in <<MaintenanceTask, MaintenanceTask daemon thread>>.
