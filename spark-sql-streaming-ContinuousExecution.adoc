== [[ContinuousExecution]] ContinuousExecution -- StreamExecution of StreamWriteSupport Sink and ContinuousTrigger

`ContinuousExecution` is a <<spark-sql-streaming-StreamExecution.adoc#, StreamExecution>> of a <<sink, StreamWriteSupport sink>> with a <<trigger, ContinuousTrigger>>.

[source, scala]
----
import org.apache.spark.sql.streaming.Trigger
import scala.concurrent.duration._
val q = spark
  .readStream
  .format("rate")
  .load
  .writeStream
  .format("console")
  .trigger(Trigger.Continuous(10.seconds))
  .queryName("rate2console")
  .option("truncate", false)
  .start

scala> q.explain
== Physical Plan ==
WriteToContinuousDataSource ConsoleWriter[numRows=20, truncate=false]
+- *(1) Project [timestamp#758, value#759L]
   +- *(1) ScanV2 rate[timestamp#758, value#759L]

// q.stop
----

`ContinuousExecution` is <<creating-instance, created>> when `StreamingQueryManager` is requested to <<spark-sql-streaming-StreamingQueryManager.adoc#createQuery, create a streaming query>> with a <<spark-sql-streaming-StreamWriteSupport.adoc#, StreamWriteSupport sink>> and a <<spark-sql-streaming-Trigger.adoc#ContinuousTrigger, ContinuousTrigger>> (when `DataStreamWriter` is requested to <<spark-sql-streaming-DataStreamWriter.adoc#start, start an execution of the streaming query>>).

[[internal-registries]]
.ContinuousExecution's Internal Properties
[cols="1m,2",options="header",width="100%"]
|===
| Name
| Description

| continuousSources
a| [[continuousSources]]

[source, scala]
----
continuousSources: Seq[ContinuousReader]
----

Collection of <<spark-sql-streaming-ContinuousReader.adoc#, ContinuousReaders>> (that are <<spark-sql-streaming-ContinuousReadSupport.adoc#, ContinuousReadSupports>> of the <<spark-sql-streaming-ContinuousExecutionRelation.adoc#, ContinuousExecutionRelations>> in the <<logicalPlan, logicalPlan>>)

NOTE: Only one continuous source is currently supported (and enforced in <<addOffset, addOffset>> and <<commit, commit>>).

Used when `ContinuousExecution` is requested to <<commit, commit>>, <<getStartOffsets, getStartOffsets>>, and <<runContinuous, runContinuous>>

Use <<sources, sources>> to access it

| currentEpochCoordinatorId
| [[currentEpochCoordinatorId]] FIXME

Used when...FIXME

| `triggerExecutor`
a| [[triggerExecutor]] <<spark-sql-streaming-TriggerExecutor.adoc#, TriggerExecutor>> for the <<trigger, Trigger>>:

* `ProcessingTimeExecutor` for `ContinuousTrigger`

Used when...FIXME

NOTE: `StreamExecution` throws an `IllegalStateException` when the <<trigger, Trigger>> is not a <<spark-sql-streaming-Trigger.adoc#ContinuousTrigger, ContinuousTrigger>>.
|===

=== [[getStartOffsets]] `getStartOffsets` Internal Method

[source, scala]
----
getStartOffsets(sparkSessionToRunBatches: SparkSession): OffsetSeq
----

`getStartOffsets`...FIXME

NOTE: `getStartOffsets` is used when...FIXME

=== [[commit]] `commit` Method

[source, scala]
----
commit(epoch: Long): Unit
----

`commit`...FIXME

NOTE: `commit` is used when...FIXME

=== [[awaitEpoch]] `awaitEpoch` Internal Method

[source, scala]
----
awaitEpoch(epoch: Long): Unit
----

`awaitEpoch`...FIXME

NOTE: `awaitEpoch` is used when...FIXME

=== [[addOffset]] `addOffset` Method

[source, scala]
----
addOffset(
  epoch: Long,
  reader: ContinuousReader,
  partitionOffsets: Seq[PartitionOffset]): Unit
----

`addOffset`...FIXME

NOTE: `addOffset` is used when...FIXME

=== [[sources]] `sources` Method

[source, scala]
----
sources: Seq[BaseStreamingSource]
----

NOTE: `sources` is part of <<spark-sql-streaming-ProgressReporter.adoc#sources, ProgressReporter Contract>> to...FIXME.

`sources`...FIXME

=== [[logicalPlan]] `logicalPlan` Value

[source, scala]
----
logicalPlan: LogicalPlan
----

NOTE: `logicalPlan` is part of <<spark-sql-streaming-StreamExecution.adoc#logicalPlan, StreamExecution Contract>> to...FIXME.

`logicalPlan`...FIXME

=== [[runActivatedStream]] Running Activated Streaming Query -- `runActivatedStream` Method

[source, scala]
----
runActivatedStream(sparkSessionForStream: SparkSession): Unit
----

NOTE: `runActivatedStream` is part of <<spark-sql-streaming-StreamExecution.adoc#runActivatedStream, StreamExecution Contract>> to run the streaming query.

`runActivatedStream`...FIXME

=== [[runContinuous]] Running Streaming Query -- `runContinuous` Internal Method

[source, scala]
----
runContinuous(sparkSessionForQuery: SparkSession): Unit
----

`runContinuous`...FIXME

NOTE: `runContinuous` is used exclusively when `ContinuousExecution` is requested to <<runActivatedStream, run the activated streaming query>>.

=== [[creating-instance]] Creating ContinuousExecution Instance

`ContinuousExecution` takes the following when created:

* [[sparkSession]] `SparkSession`
* [[name]] The name of the structured query
* [[checkpointRoot]] Path to the checkpoint directory (aka _metadata directory_)
* [[analyzedPlan]] Analyzed logical query plan (`LogicalPlan`)
* [[sink]] <<spark-sql-streaming-StreamWriteSupport.adoc#, StreamWriteSupport>>
* [[trigger]] <<spark-sql-streaming-Trigger.adoc#, Trigger>>
* [[triggerClock]] `Clock`
* [[outputMode]] <<spark-sql-streaming-OutputMode.adoc#, Output mode>>
* [[extraOptions]] Options (`Map[String, String]`)
* [[deleteCheckpointOnStop]] `deleteCheckpointOnStop` flag to control whether to delete the checkpoint directory on stop

`ContinuousExecution` initializes the <<internal-registries, internal registries and counters>>.

=== [[stop]] Stopping Streaming Query -- `stop` Method

[source, scala]
----
stop(): Unit
----

NOTE: `stop` is part of the <<spark-sql-streaming-StreamingQuery.adoc#stop, StreamingQuery Contract>> to stop the streaming query.

`stop` transitions the streaming query to `TERMINATED` state.

If the <<spark-sql-streaming-StreamExecution.adoc#queryExecutionThread, queryExecutionThread>> is alive (i.e. it has been started and has not yet died), `stop` interrupts it and waits for this thread to die.

In the end, `stop` prints out the following INFO message to the logs:

```
Query [prettyIdString] was stopped
```

NOTE: <<spark-sql-streaming-StreamExecution.adoc#prettyIdString, prettyIdString>> is in the format of `queryName [id = [id], runId = [runId]]`.
