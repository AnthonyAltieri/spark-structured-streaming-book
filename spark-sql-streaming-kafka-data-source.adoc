== Kafka Data Source -- Streaming Data Source for Apache Kafka

*Kafka Data Source* is a streaming data source for https://kafka.apache.org/[Apache Kafka].

=== External Module

Kafka Data Source is an external module that is distributed as part of the official distribution of Apache Spark, but is not available on the CLASSPATH by default. You should define it as part of a build definition (e.g. as a `libraryDependency` in `build.sbt` for sbt) or use `--packages` command-line option when `spark-submit` a Spark Structured Streaming application.

All the examples that follow assume that you started `spark-shell` with the `--packages` command-line option.

[source, scala]
----
./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.12:{{ book.version }}
----

=== kafka Format

Kafka Data Source is registered under *kafka* data source format (via <<spark-sql-streaming-KafkaSourceProvider.adoc#, KafkaSourceProvider>> and the corresponding `META-INF/services/org.apache.spark.sql.sources.DataSourceRegister` service registration file).

[source, scala]
----
val inputs = spark
  .readStream
  .format("kafka") // <-- uses Kafka data source
  .option("subscribepattern", "input*")
  .option("kafka.bootstrap.servers", ":9092")
  .load
scala> inputs.explain(extended = true)
== Parsed Logical Plan ==
StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@1acb7cf0, kafka, Map(subscribepattern -> input*, kafka.bootstrap.servers -> :9092), [key#4344, value#4345, topic#4346, partition#4347, offset#4348L, timestamp#4349, timestampType#4350], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1b33bbbe,kafka,List(),None,List(),None,Map(subscribepattern -> input*, kafka.bootstrap.servers -> :9092),None), kafka, [key#4337, value#4338, topic#4339, partition#4340, offset#4341L, timestamp#4342, timestampType#4343]
...
----

NOTE: <<spark-sql-streaming-KafkaSourceProvider.adoc#, KafkaSourceProvider>> is the provider for streaming <<spark-sql-streaming-StreamSourceProvider.adoc#, sources>> and <<spark-sql-streaming-StreamSinkProvider.adoc#, sinks>> for the Kafka data source.

=== Continuous Stream Processing

Kafka Data Source supports <<spark-sql-streaming-continuous-stream-processing.adoc#, Continuous Stream Processing>> (i.e. <<spark-sql-streaming-Trigger.adoc#Continuous, Trigger.Continuous>> trigger) via <<spark-sql-streaming-KafkaContinuousReader.adoc#, KafkaContinuousReader>>.

[source, scala]
----
import org.apache.spark.sql.streaming.Trigger
import scala.concurrent.duration._
val sq = spark
  .readStream
  .format("kafka")
  .option("subscribepattern", "input*")
  .option("kafka.bootstrap.servers", ":9092")
  .load
  .writeStream
  .format("console")
  .option("truncate", false)
  .trigger(Trigger.Continuous(15.seconds))
  .queryName("kafka2console")
  .start
----

=== Micro-Batch Stream Processing

Kafka Data Source supports <<spark-sql-streaming-micro-batch-processing.adoc#, Micro-Batch Stream Processing>> (i.e. <<spark-sql-streaming-Trigger.adoc#Once, Trigger.Once>> and <<spark-sql-streaming-Trigger.adoc#ProcessingTime, Trigger.ProcessingTime>> triggers) via <<spark-sql-streaming-KafkaMicroBatchReader.adoc#, KafkaMicroBatchReader>>.
